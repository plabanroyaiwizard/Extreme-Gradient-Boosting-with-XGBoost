{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the number of boosting rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_boosting_rounds          rmse\n",
    "0                    5  50903.298177\n",
    "1                   10  34774.191406\n",
    "2                   15  32895.098958"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated boosting round selection using early_stopping\n",
    "\n",
    "\n",
    "Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within xgb.cv(). This is done using a technique called early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=50, early_stopping_rounds=10, metrics=\"rmse\", as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
    "0     141871.635417      403.636200   142640.656250     705.559400\n",
    "1     103057.033854       73.772531   104907.664062     111.112417\n",
    "2      75975.966146      253.726099    79262.057291     563.767892\n",
    "3      57420.532552      521.658345    61620.134115    1087.692518\n",
    "4      44552.955729      544.170190    50437.561198    1846.446330\n",
    "5      35763.946615      681.797429    43035.661458    2034.469207\n",
    "6      29861.463542      769.572072    38600.880208    2169.796232\n",
    "7      25994.676432      756.520565    36071.817708    2109.795430\n",
    "8      23306.836588      759.238254    34383.184896    1934.546688\n",
    "9      21459.770182      745.625311    33509.141276    1887.375284\n",
    "10     20148.721354      749.612769    32916.808594    1850.894249\n",
    "11     19215.382162      641.387014    32197.832682    1734.456935\n",
    "12     18627.388672      716.257510    31770.852865    1802.155484\n",
    "13     17960.694661      557.043073    31482.781250    1779.124592\n",
    "14     17559.736328      631.412555    31389.992188    1892.321520\n",
    "15     17205.712565      590.171852    31302.882162    1955.165902\n",
    "16     16876.571940      703.631755    31234.058594    1880.705796\n",
    "17     16597.662110      703.677609    31318.348308    1828.860391\n",
    "18     16330.460937      607.274494    31323.634766    1775.910706\n",
    "19     16005.972331      520.470326    31204.134766    1739.077059\n",
    "20     15814.301432      518.604477    31089.862630    1756.021674\n",
    "21     15493.405924      505.615987    31047.996094    1624.673955\n",
    "22     15270.733724      502.019237    31056.916015    1668.042812\n",
    "23     15086.381836      503.912899    31024.984375    1548.985605\n",
    "24     14917.608399      486.206187    30983.684896    1663.130201\n",
    "25     14709.589518      449.668010    30989.477214    1686.666560\n",
    "26     14457.286133      376.787666    30952.113932    1613.172643\n",
    "27     14185.567383      383.102234    31066.902344    1648.534310\n",
    "28     13934.067383      473.465256    31095.640625    1709.225072\n",
    "29     13749.644857      473.670302    31103.886719    1778.879529\n",
    "30     13549.836914      454.898141    30976.085287    1744.514533\n",
    "31     13413.485351      399.603618    30938.469401    1746.052597\n",
    "32     13275.916341      415.408908    30931.001953    1772.469510\n",
    "33     13085.877930      493.792427    30929.057291    1765.540568\n",
    "34     12947.181640      517.789318    30890.630208    1786.511479\n",
    "35     12846.027018      547.732021    30884.492187    1769.728223\n",
    "36     12702.379232      505.523221    30833.541667    1691.002563\n",
    "37     12532.243490      508.298594    30856.688151    1771.446377\n",
    "38     12384.054362      536.225108    30818.016276    1782.784214\n",
    "39     12198.444010      545.165502    30839.392578    1847.327022\n",
    "40     12054.583659      508.841772    30776.966146    1912.780507\n",
    "41     11897.036458      477.177568    30794.702474    1919.674832\n",
    "42     11756.221680      502.992782    30780.955078    1906.820029\n",
    "43     11618.846029      519.836706    30783.755860    1951.260705\n",
    "44     11484.080404      578.427828    30776.731120    1953.446309\n",
    "45     11356.553060      565.368827    30758.543620    1947.454953\n",
    "46     11193.558594      552.298906    30729.971354    1985.700237\n",
    "47     11071.315429      604.089960    30732.663411    1966.997822\n",
    "48     10950.778646      574.863135    30712.241536    1957.751573\n",
    "49     10824.865885      576.665756    30720.854818    1950.511514"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It iss time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! We'll begin by tuning the \"eta\", also known as the learning rate.\n",
    "\n",
    "The learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta\n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,\n",
    "                        num_boost_round=10, early_stopping_rounds=5,\n",
    "                        metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tune max_depth, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to.\n",
    "Smaller values will lead to shallower trees, and larger values to deeper trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning colsample_bytree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tune \"colsample_bytree\". \n",
    "We've already seen this if you've ever worked with scikit-learn's RandomForestClassifier or RandomForestRegressor, \n",
    "where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params[\"colsample_bytree\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " colsample_bytree     best_rmse\n",
    "0               0.1  48193.451172\n",
    "1               0.5  36013.544922\n",
    "2               0.8  35932.962891\n",
    "3               1.0  35836.044922"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
    "                        scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 50}\n",
    "Lowest RMSE found:  29916.562522854438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, \n",
    "as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV\n",
    "code to do RandomizedSearchCV. The key difference is you have to specify a param_distributions parameter instead\n",
    "of a param_grid parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,\n",
    "                                    n_iter=5, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \",randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best parameters found:  {'n_estimators': 25, 'max_depth': 6}\n",
    "Lowest RMSE found:  36909.98213965752"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
